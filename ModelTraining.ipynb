{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS598 Deep Learning for Healthcare - Final Project\n",
    "\n",
    "*Author*: Michael Haines | mhaines2@illinois.edu\n",
    "\n",
    "The code trains extracts a large dataset and trains several BERT-based models. It will take several hours to run to completion and should be used in an environment that has access to a GPU. All data and code should be uploaded to Google Drive to make it easy to run from Google Colab.\n",
    "\n",
    "## Pre-requisites\n",
    "\n",
    "### Upload the Data to Google Drive\n",
    "\n",
    "Assuming that you have cloned the repo and followed the instructions in the `README.md` to add the MIMIC-IV dataset to the correct folder, upload the whole repo into your Google drive. It should have the following structure at the minimum to run to completion:\n",
    "\n",
    "```\n",
    "├── data\n",
    "│   ├── full\n",
    "│   │   ├── dict\n",
    "│   │   └── tokens\n",
    "│   ├── sample\n",
    "│   │   ├── dict\n",
    "│   │   └── tokens\n",
    "├── models\n",
    "│   ├── behrt_finetune_model.py\n",
    "│   ├── behrt_finetune.py\n",
    "│   ├── behrt_model.py\n",
    "│   ├── behrt_no_d_model.py\n",
    "│   ├── behrt_no_d_train.py\n",
    "│   ├── behrt_pretrain_model.py\n",
    "│   ├── behrt_pretrain.py\n",
    "│   └── behrt_train.py\n",
    "├── requirements.txt   \n",
    "```\n",
    "Note that it will take some time to upload this data to Colab, as the saved datasets are quite large.\n",
    "\n",
    "### Mount Google  Drive\n",
    "\n",
    "Please use Google Colab to run the notebook, as this will ensure that the runtime has access to your Google Drive. Mount the Google Drive with the following code. The code assumes that you have uploaded the the cloned repo to the root directory for your Google Drive. If you have uploaded the repo to a subdirectory, change the `PROJECT_ROOT` accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = \"cs598-final-project\" # Change this if required\n",
    "\n",
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# Change working directory\n",
    "os.chdir(f'/content/drive/{PROJECT_ROOT}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install & Import Runtime Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Adding modules\n",
    "warnings.filterwarnings('ignore')\n",
    "module_path='./models'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Model training\n",
    "import behrt_train\n",
    "importlib.reload(behrt_train)\n",
    "import behrt_train\n",
    "from behrt_train import *\n",
    "import behrt_no_d_train\n",
    "importlib.reload(behrt_no_d_train)\n",
    "import behrt_no_d_train\n",
    "from behrt_no_d_train import *\n",
    "import behrt_pretrain\n",
    "importlib.reload(behrt_pretrain)\n",
    "import behrt_pretrain\n",
    "from behrt_pretrain import *\n",
    "import behrt_finetune\n",
    "importlib.reload(behrt_finetune)\n",
    "import behrt_finetune\n",
    "from behrt_finetune import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models\n",
    "\n",
    "### Model #1: Public Checkpoint Fine-tuned on Sample Cohort\n",
    "\n",
    "We are training this model as a baseline for comparison. It uses a pre-trained checkpoint BERT model available from the [pytorch-pretrained-bert package](https://pypi.org/project/pytorch-pretrained-bert/) and fine-tunes on the EHR data.\n",
    "\n",
    "We first train the model on a small sample to test the researchers' premise that BERT-based models trained on EHR data require large samples to be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"sample\"\n",
    "tokenized_src = pd.read_csv(f'./data/{path}/tokens/tokenized_src.csv', index_col=0)\n",
    "tokenized_age = pd.read_csv(f'./data/{path}/tokens/tokenized_age.csv', index_col=0)\n",
    "tokenized_gender = pd.read_csv(f'./data/{path}/tokens/tokenized_gender.csv', index_col=0)\n",
    "tokenized_ethni = pd.read_csv(f'./data/{path}/tokens/tokenized_ethni.csv', index_col=0)\n",
    "tokenized_ins = pd.read_csv(f'./data/{path}/tokens/tokenized_ins.csv', index_col=0)\n",
    "tokenized_labels = pd.read_csv(f'./data/{path}/tokens/tokenized_labels.csv', index_col=0)\n",
    "behrt_train.train_behrt(tokenized_src, tokenized_age, tokenized_gender, tokenized_ethni, tokenized_ins, tokenized_labels, path=path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #2: Public Checkpoint Fine-tuned on Full Cohort\n",
    "\n",
    "This is the same model as #1, but trained on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"full\"\n",
    "tokenized_src = pd.read_csv(f'./data/{path}/tokens/tokenized_src.csv', index_col=0)\n",
    "tokenized_age = pd.read_csv(f'./data/{path}/tokens/tokenized_age.csv', index_col=0)\n",
    "tokenized_gender = pd.read_csv(f'./data/{path}/tokens/tokenized_gender.csv', index_col=0)\n",
    "tokenized_ethni = pd.read_csv(f'./data/{path}/tokens/tokenized_ethni.csv', index_col=0)\n",
    "tokenized_ins = pd.read_csv(f'./data/{path}/tokens/tokenized_ins.csv', index_col=0)\n",
    "tokenized_labels = pd.read_csv(f'./data/{path}/tokens/tokenized_labels.csv', index_col=0)\n",
    "behrt_train.train_behrt(tokenized_src, tokenized_age, tokenized_gender, tokenized_ethni, tokenized_ins, tokenized_labels, path=path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #3 Public Checkpoint Fine-tuned on Full Cohort, Demographic Data Excluded\n",
    "\n",
    "This is the same as Model #2, with the potentially sensitive data `age`, `insurance`, `ethnicity`, and `gender` excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"full\"\n",
    "tokenized_src = pd.read_csv(f'./data/{path}/tokens/tokenized_src.csv', index_col=0)\n",
    "tokenized_labels = pd.read_csv(f'./data/{path}/tokens/tokenized_labels.csv', index_col=0)\n",
    "behrt_model_no_d.train_behrt(tokenized_src, tokenized_labels, path=path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training BERT from Full Cohort\n",
    "\n",
    "In this model, we use the raw BERT head from [pytorch-pretrained-bert package](https://pypi.org/project/pytorch-pretrained-bert/) and perform the masked language modeling task ourself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"full\"\n",
    "behrt_pretrain.pretrain_behrt(path=path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #4: Custom Checkpoint Fine-tuned on Full Cohort in Adversarial Setting\n",
    "\n",
    "In this model, we use the pre-trained checkpoint that we just created and perform the fine-tuning task to replicate the model that from the original study. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"full\"\n",
    "behrt_finetune.finetune_behrt(path=path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
